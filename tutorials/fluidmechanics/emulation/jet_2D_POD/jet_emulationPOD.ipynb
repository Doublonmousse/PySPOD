{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Jet POD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore a small dataset provided with this package that contains pressure data of the flow exiting a nozzle (also referred to as a jet).\n",
    "Cylindrical coordinates (r,x) are used and they are equally spaced. \n",
    "In particular, starting from a database of pre computed solutions, we want to:\n",
    "- extract the POD (coherent in space) modes\n",
    "- compute the coefficients projecting the data on the POD basis built by gathering the modes\n",
    "- learn the latent dynamics through a LSTM-based neural network\n",
    "- predict future coefficients, i.e. the future evolution in time of the flow field\n",
    "- compare predictions with the actual flow realizations\n",
    "\n",
    "In detail, the starting dataset consists of 1000 flow realizations which represent the pressure field at different time instant. The time step is 12 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is part of the data used for the regression tests that come with this library and is stored into `tests/data/fluidmechanic_data.mat`. The first step to anlyze this dataset is to import the required libraries, including the custom libraries\n",
    "- 'from pyspod.pod_base import POD_base'\n",
    "- 'from pyspod.emulation   import Emulation'\n",
    "- 'import pyspod.utils_weights as utils_weights'\n",
    "- 'import pyspod.utils as utils'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Current, parent and file paths import sys\n",
    "CWD = os.getcwd()\n",
    "sys.path.insert(0, os.path.join(CWD, \"../../../../\"))\n",
    "\n",
    "# Import specific libraries\n",
    "from pyspod.pod_base import POD_base\n",
    "from pyspod.emulation   import Emulation\n",
    "import pyspod.utils_weights as utils_weights\n",
    "import pyspod.utils as utils  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step conists into importing data from \"fluidmechanics_data.mat\" located in the folder pysod/test/data and store them in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(CWD,'../../../../tests/data','fluidmechanics_data.mat')\n",
    "variables = ['p']\n",
    "with h5py.File(file, 'r') as f:\n",
    "\tdata_arrays = dict()\n",
    "\tfor k, v in f.items():\n",
    "\t\tdata_arrays[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables and global parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored in a matrix `X` and, to be suitable to the `PySPOD` library, it must have the following features:\n",
    "- first dimension must correspond to the number of time snapshots (1000 in our case)\n",
    "- last dimension should corresponds to the number of variables (1 in our case)\n",
    "- the remaining dimensions corresponds to the spatial dimensions (20, and 88 in our case, that correspond to radial and axial spatial coordinates).\n",
    "We note that in the present test case the data matrix `X` used is already in a shape that is suitable to `PySPOD`, as its dimension is:\n",
    "$$\\text{$X$ dimensions} = 1000 \\times 20 \\times 88 $$\n",
    "\n",
    "Other global variables and parameters are defined. In detail:\n",
    "- `nt`: integer, number of snapshots, read from the data_arrays\n",
    "- `t`: vector containing the time instants at which the flow realizations have been stored\n",
    "- `x1`: list of coordinate along the x1-axis (in this case x1='r')\n",
    "- `x2`: list of coordinates along the x2-axis(in this case x2='x')\n",
    "- `trainingDataRatio`: real, ratio between training data number and total number of snapshots\n",
    "\n",
    "In the dictionary 'params' the following variables are stored:\n",
    "- `dt`: time step\n",
    "- `n_space_dims`: integer, number of space dimensions, in our case 2\n",
    "- `n_variables`: integer, nr of variables, in our case 1\n",
    "- `n_modes_save`: integer, number of modes that are taken into account (and stored)\n",
    "- `savedir`: string, name of the directory where results will be saved\n",
    "- `normalize_weights` (optional): boolean which activate the normalization of weights by data variance\n",
    "- `normalize_data` (optional): boolean which normalize data by data variance\n",
    "\n",
    "**Note that we do not set any parameter for the Weights adopted to compute th einner product in the SPOD calculation. In this case, the algorithm will use automatically uniform weighting (weighting equal 1), and it will prompt a warning stating the use of default uniform weighting.** \n",
    "\n",
    "In the dictionary 'params_emulation' the following variables, which allow to define some relevant parameters of a single layer neural network, are stored:\n",
    "- `network`: string, type of neural network. In the present tutorial 'lstm'\n",
    "- `epochs`: integer, number of epochs\n",
    "- `batch_size`: integer, batch size\n",
    "- `n_seq_in`: integer, dimension of input sequence \n",
    "- `n_seq_out`: integer, number of steps to predict\n",
    "- `n_neurons`: number of neurons in each layer\n",
    "- `dropout`: value of the dropout\n",
    "- `savedir`: string, name of the directory where results will be saved\n",
    "\n",
    "In the present test case we use 60 (`n_seq_in`=60) previous values of the coefficients in order to evaluate the next one (`n_seq_out`=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data_arrays['dt'][0,0]\n",
    "X = data_arrays[variables[0]].T\n",
    "\n",
    "t = dt * np.arange(0,X.shape[0]); t = t.T\n",
    "nt = t.shape[0]\n",
    "x1 = data_arrays['r'].T; x1 = x1[:,0]\n",
    "x2 = data_arrays['x'].T; x2 = x2[0,:]\n",
    "\n",
    "trainingDataRatio = 0.8 # ratio between training data number and total number of snapshots\n",
    "testingDataRatio = (1-trainingDataRatio)\n",
    "\n",
    "# parameters\n",
    "params = dict()\n",
    "\n",
    "# -- required parameters\n",
    "params['time_step'   \t   ] = dt \t\t\t\t\t\t# data time-sampling\n",
    "params['n_space_dims'    ] = 2\t\t\t\t\t\t\t# number of spatial dimensions (longitude and latitude)\n",
    "params['n_variables'     ] = 1\t\t\t\t\t\t\t# number of variables\n",
    "# -- optional parameters\n",
    "params['normalize_weights'] = False\t \t\t\t# normalization of weights by data variance\n",
    "params['normalize_data'   ] = False  \t\t\t# normalize data by data variance\n",
    "params['n_modes_save'     ] = 8  \t\t# modes to be saved\n",
    "params['savedir'          ] = os.path.join(CWD, 'results', Path(file).stem)\n",
    "\n",
    "params_emulation = dict()\n",
    "\n",
    "params_emulation['network'     ] = 'lstm' \t\t\t\t\t\t# type of network\n",
    "params_emulation['epochs'      ] = 130 \t\t\t\t\t\t# number of epochs\n",
    "params_emulation['batch_size'  ] = 32\t\t\t\t\t\t\t# batch size\n",
    "params_emulation['n_seq_in'    ] = 60\t\t\t\t\t\t\t# dimension of input sequence \n",
    "params_emulation['n_seq_out'   ] = 1                          # number of steps to predict\n",
    "params_emulation['n_neurons'   ] = 25                          # number of neurons\n",
    "params_emulation['dropout'   ] = 0.15                          # dropout\n",
    "params_emulation['savedir'     ] = os.path.join(CWD, 'results', Path(file).stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute POD modes and coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code are used for the initialization of variables. `X_train` and `X_test` are numpy data structures which contain the training set and the testing set respectively; therefore their dimensions are (`nt_train`, 20, 88) and (`nt_test`, 20, 88), being in this test case `nt_train`= $0.8\\cdot nt$ and nt_test=$0.2\\cdot nt$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training and testing database definition\n",
    "nt_train = int(trainingDataRatio * nt)\n",
    "X_train = X[:nt_train,:,:]\n",
    "nt_test = nt - nt_train\n",
    "X_test  = X[nt_train:,:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the data, defined the required and optional parameters, and allocated the testing and training structures we can perform the analysis. This step is accomplished by calling the `PySPOD` constructor`POD_base(params=params, data_handler=False, variables=variables)` and the `fit` method, `POD_analysis.fit(data=X_train, nt=nt_train)`. \n",
    "\n",
    "The `POD_base` constructor takes the following arguments :\n",
    "  - `params`: must be a dictionary and contains the parameters that we have just defined. \n",
    "  - `data_handler`: can be either `False` or a function handler. If it is a function handler, it must hold the function to read the data. The template for the function to read the data must have as first argument the data file, as second and third the time indices through which we will slice the data in time, and as fourth argument a list containing the name of the variables. See our data reader as an example and modify it according to your needs.\n",
    "  - `variables`: is a list containing our variables. \n",
    "The function `fit` returns a reference to the instance object on which it was called, given data and size of this dataset as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPOD analysis\n",
    "POD_analysis = POD_base(\n",
    "params=params, \n",
    "data_handler=False, \n",
    "variables=variables\n",
    ")\n",
    "# fit \n",
    "pod = POD_analysis.fit(data=X_train, nt=nt_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transform function the pressure fluctuations are computed by subtracting the mean field from the snapshots. Then the POD modes are evaluated and the coefficients are obtained by projecting the snapshots representing the pressure fluctuations onto the reduced POD basis obtained by gathering the most significant modes. In details, the `pod.transform` function accept as input\n",
    "- `data`: dataset on which the analysis is performed\n",
    "- `nt`: number of snapshots of the dataset 'data'\n",
    "\n",
    "and it returns a dictionary which contains the following keywords:\n",
    "- `time_mean`: the average in time of snapshots\n",
    "- `phi_tilde`: the first most significant modes, i.e. the ones associated to the biggest n_save_modes eigenvalues. These modes identify a reduced basis, significant for the case at hand.\n",
    "- `coeffs`: the coefficients obtained by projection\n",
    "- `reconstructed_data`: snapshots reconstructed by superimposing the modes multiplied by the coefficients\n",
    "\n",
    "The coeffs_test is instead a vector which contains the coefficients which are evaluated by projecting the snapshots of the testing database onto the reduced POD basis previously computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "coeffs_train = pod.transform(data=X_train, nt=nt_train)\n",
    "\n",
    "X_rearrange_test = np.reshape(X_test[:,:,:], [nt_test,pod.nv*pod.nx])\n",
    "for i in range(nt_test):\n",
    "\tX_rearrange_test[i,:] = np.squeeze(X_rearrange_test[i,:]) - np.squeeze(coeffs_train['time_mean'])\n",
    "coeffs_test = np.matmul(np.transpose(coeffs_train['phi_tilde']), X_rearrange_test.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the latent space dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines are required in order to initialize the data structures needed to train the neural network and to store its output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of variables and structures\n",
    "n_modes = params['n_modes_save'] \n",
    "n_feature = coeffs_train['coeffs'].shape[0]\n",
    "\n",
    "data_train = np.zeros([n_modes,coeffs_train['coeffs'].shape[1]],dtype='double')\n",
    "data_test = np.zeros([n_modes,coeffs_test.shape[1]],dtype='double')\n",
    "coeffs_emul = np.zeros([coeffs_test.shape[0],coeffs_test.shape[1]],dtype='double')\n",
    "coeffs_tmp = np.zeros([n_modes,coeffs_test.shape[1]],dtype='double')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients previously evaluated can now be used for training a LSTM-based neural network.\n",
    "The Emulation constructor requires the following parameters:\n",
    "- `params_emulation`: dict containing the parameters described in the previous sections. They contain all the relevant data for creating a single-layer neural network with Dropout\n",
    "\n",
    "The neural network is initialized by calling `pod.model_initialize` that requires the data set which the network will be trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "pod_emulation = Emulation(params_emulation)\n",
    "\n",
    "# initialization of the network\n",
    "pod_emulation.model_initialize(data=data_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to provide scaled input to the neural network. For this reason a scaler vector is computed by calling the function `utils.compute_normalizationVectorReal`. Three different arguments can be used for defining the `normalizeMethod` variable:\n",
    "- `localmax`: each coefficient is scaled by its local maximum\n",
    "- `globalmax`: all the coefficients are scaled by the same value which represent the global maximum\n",
    "- `None`: no scaling is applied. The output vector contains ones.\n",
    "Once that the scaling vector is known, the scaling is applied both to the training dataset and to the testing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and normalize data \n",
    "scaler  = utils.compute_normalizationVectorReal(coeffs_train['coeffs'][:,:], normalizeMethod='localmax')\n",
    "data_train[:,:] = utils.normalize_dataReal(coeffs_train['coeffs'][:,:], normalizationVec=scaler)\n",
    "data_test[:,:]  = utils.normalize_dataReal(coeffs_test[:,:], normalizationVec=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the neural network is carried out by calling the `pod_emulation.model_train` function. The following inputs are requested:\n",
    "- `idx`: integer, it is an identifier associated to the neural network. Thanks to this idx, more than one network can be trained in the same run and the weights can be stored in different files.\n",
    "- `data_train`: dataset used for the training\n",
    "- `data_valid`: dataset used for the validation\n",
    "- `plotHistory` (otpional): boolean, plot  the trainig history when set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "# train the network\n",
    "pod_emulation.model_train(idx,\n",
    " \tdata_train=data_train, \n",
    " \tdata_valid=data_test,\n",
    " \tplotHistory=False\n",
    " \t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that the neural network has been trained, predictions of the coefficients can be extracted with the aid of the `pod_emulation.model_inference` routine. This receives as inputs:\n",
    "- `idx`: integer, a value which identify a previously trained neural network (in this case 0, since we have only one neural network)\n",
    "- `data_input`: data which are used to start the prediction. This array can have an arbitary length. The first `n_seq_in` data are copied in the output vector and used for predicting the next `n_seq_out` steps\n",
    "\n",
    "The output consists in a vector which has the same dimensions of data_input and contains the predicted scaled coefficients.\n",
    "\n",
    "The predicted coefficients are then scaled back by calling `utils.denormalize_dataReal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_tmp = pod_emulation.model_inference(\n",
    "\tidx, data_input=data_test\n",
    "\t)\n",
    "coeffs_emul[:,:] = utils.denormalize_dataReal(coeffs_tmp, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two distinct types of coefficients which we can use for reconstructing the snaptshots contained in X_test:\n",
    "- `coeffs_test`: the ones which were obtained by projecting the snapshot on the POD basis\n",
    "- `emul_coeffs`: the ones which were obtained with the prediction of the LSTM-based neural network.\n",
    "\n",
    "Fields are reconstructed and stored in a proper numpy array by calling `reconstruct_data` and providing the following input:\n",
    "- `coeffs`: the coefficients to be used for reconstructing the fields\n",
    "- `phi_tilde`: a structure containing the modes computed in the `transform` function\n",
    "- `time_mean`: the mean flow previously computed with the `transform` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_tilde = coeffs_train['phi_tilde']\n",
    "time_mean = coeffs_train['time_mean']\n",
    "\n",
    "proj_rec = pod.reconstruct_data(\n",
    "\t\tcoeffs=coeffs_test[:,:], \n",
    "\t\tphi_tilde=coeffs_train['phi_tilde'],\n",
    "\t\ttime_mean=coeffs_train['time_mean']\n",
    "\t)\n",
    "\n",
    "emulation_rec = pod.reconstruct_data(\n",
    "\t\tcoeffs= coeffs_emul, \n",
    "\t\tphi_tilde=coeffs_train['phi_tilde'],\n",
    "\t\ttime_mean=coeffs_train['time_mean']\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section of the code, some routines are placed for visulizing some results and computing the errors.\n",
    "\n",
    "`pod.printErrors`: compute and print L1, L2, and $L_{\\inf}$ average norm error for both the learning and the projection error. \n",
    "In intput the following input are required:\n",
    "- `field_test`: \"true\" solutions, it is a snapshot which belong to the original dataset \n",
    "- `field_proj`: fields reconstructed using the coeffs_test, from the comparison between this database and the one containing the true solutions we can evalute the projection error\n",
    "- `field_emul`: fields reconstructed using the coeffs_emul; from the comparison between this database and the field_proj we can evalute the learning error; from the comparison between this database and the one containing the true solutions we can evalute the total error.\n",
    "- `n_snaps`: number of snapshots on which the errors are evaluated \n",
    "- `n_offset`: offset\n",
    "\n",
    "`pod.plot_compareTimeSeries`: compare time series, it is here used for comparing actual coefficients and the learned ones. It requires in input: two time series, two labels of the time series, legendLocation(otpional), and the filename (optional)\n",
    "\n",
    "`pod.generate_2D_subplot`: generate subpolts for visualizing the snapshots. It can show 1, 2 or 3 fields in the same frame. Inputs:\n",
    "- `title1`: title associated to the first snapshot\n",
    "- `title2`: title associated to the second snapshot (optional)\n",
    "- `title3`: title associated to the third snapshot (optional)\n",
    "- `var1`: 2D array of dimenions $20\\times88$ that we want to plot\n",
    "- `var2`: 2D array of dimenions $20\\times88$ that we want to plot (optional)\n",
    "- `var3`: 2D array of dimenions $20\\times88$ that we want to plot (optional)\n",
    "- `N_round`: number of decimals one wants to keep in the legend (optional)\n",
    "- `path`: path where one wants to store the results(optional)\n",
    "- `filename`: name of the file where to save the plot(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod.printErrors(field_test=X_test, field_proj=proj_rec, field_emul=emulation_rec, n_snaps = 100, n_offset = 100)\n",
    "\n",
    "# routines for visualization\n",
    "pod.plot_compareTimeSeries(\n",
    "\t\t\t  serie1= coeffs_test[0,:],\n",
    "\t\t\t  serie2= coeffs_emul[0,:],\n",
    "\t\t\t  label1='test',\n",
    "\t\t\t  label2='lstm',\n",
    "\t\t\t  legendLocation = 'upper left',\n",
    "\t\t\t  filename=None)\n",
    "\n",
    "pod.generate_2D_subplot(\n",
    "\ttitle1='True solution', \n",
    "\ttitle2='Projection-based solution', \n",
    "\ttitle3='LSTM-based solution',\n",
    "\tvar1=X_test[100,:,:], \n",
    "\tvar2=proj_rec[100,:,:,0], \n",
    "\tvar3=emulation_rec[100,:,:,0], \n",
    "\tN_round=2, path='CWD', filename=None\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
