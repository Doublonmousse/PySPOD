{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Jet SPOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore a small dataset provided with this package that contains pressure data of the flow exiting a nozzle (also referred to as a jet).\n",
    "Cylindrical coordinates (r,x) are used and they are equally spaced. \n",
    "In particular, starting from a database of pre computed solutions, we want to:\n",
    "- extract the SPOD (coherent in space and time) modes\n",
    "- compute the coefficients projecting the data on the SPOD basis built by gathering the modes\n",
    "- learn the latent dynamics through a LSTM-based neural network\n",
    "- predict future coefficients, i.e. the future evolution in time of the flow field\n",
    "- compare predictions with the actual flow realizations\n",
    "\n",
    "In detail, the starting dataset consists of 1000 flow realizations which represent the pressure field at different time instant. The time step is 12 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is part of the data used for the regression tests that come with this library and is stored into `tests/data/fluidmechanic_data.mat`. The first step to anlyze this dataset is to import the required libraries, including the custom libraries\n",
    "- 'from pyspod.spod_low_storage import SPOD_low_storage'\n",
    "- 'from pyspod.spod_low_ram     import SPOD_low_ram'\n",
    "- 'from pyspod.spod_streaming   import SPOD_streaming'\n",
    "- 'from pyspod.emulation   import Emulation'\n",
    "- 'import pyspod.utils_weights as utils_weights'\n",
    "- 'import pyspod.utils as utils'.  \n",
    "\n",
    "The first three contain different implementations of the SPOD algorithm, the first requiring low storage memory (intended for large RAM machines or small amount of data), the second requiring low RAM (intended for large dataset or small RAM machines), and the third being a streaming algorithm, that required little amount of memory (both storage and RAM) but runs typically slower than the other two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Current, parent and file paths import sys\n",
    "CWD = os.getcwd()\n",
    "sys.path.insert(0, os.path.join(CWD, \"../../../../\"))\n",
    "\n",
    "# Import specific libraries\n",
    "from pyspod.spod_low_storage import SPOD_low_storage\n",
    "from pyspod.spod_low_ram     import SPOD_low_ram\n",
    "from pyspod.spod_streaming   import SPOD_streaming\n",
    "from pyspod.emulation   import Emulation\n",
    "import pyspod.utils_weights as utils_weights\n",
    "import pyspod.utils as utils  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step conists into importing data from \"fluidmechanics_data.mat\" located in the folder pysod/test/data and store them in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(CWD,'../../../../tests/data','fluidmechanics_data.mat')\n",
    "variables = ['p']\n",
    "with h5py.File(file, 'r') as f:\n",
    "\tdata_arrays = dict()\n",
    "\tfor k, v in f.items():\n",
    "\t\tdata_arrays[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables and global parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored in a matrix `X` and, to be suitable to the `PySPOD` library, it must have the following features:\n",
    "- first dimension must correspond to the number of time snapshots (1000 in our case)\n",
    "- last dimension should corresponds to the number of variables (1 in our case)\n",
    "- the remaining dimensions corresponds to the spatial dimensions (20, and 88 in our case, that correspond to radial and axial spatial coordinates).\n",
    "We note that in the present test case the data matrix `X` used is already in a shape that is suitable to `PySPOD`, as its dimension is:\n",
    "$$\\text{$X$ dimensions} = 1000 \\times 20 \\times 88 $$\n",
    "\n",
    "\n",
    "It is important to note at this point that we loaded all the required data into RAM, and stored it into a `numpy.ndarray`. We will later pass this array to the constructor of the `PySPOD` class for running our analysis. However, we could have used a different approach to load the data. In fact, the constructor to the `PySPOD` class accepts an argument called `data_handler`, that points to a function whose objective is to read the data at run time. This is particularly useful for large datasets, where it might be not possible to load all the data in RAM upfront. Therefore, in this case, we could simply define a data reader function as the following:\n",
    "\n",
    "```\n",
    "def read_data(data, t_0, t_end, variables): \n",
    "    ... implement here your method\n",
    "    data: path to the data file\n",
    "    t_0: start time slicing\n",
    "    t_end: end time slicing\n",
    "    variables: list with names of the variables\n",
    "\n",
    "    return X\n",
    "```\n",
    "\n",
    "and pass it to the `PySPOD` constructor under the argument `data_handler`. The path to the data file, will then be specified in place of the data, under the argument `X`. See below, when we setup the analysis and call the constructor for a more detailed explantion of the parameters `X` and `data_handler`. In summary, if `X` is a numpy.ndarray containing your data, `data_handler` is set to `False`, if `X` is a `str` containing the path to your data file, `data_handler` is a function that reads your data, and whose arguments must be: (1.) `str` containing the path to the data file, (2) `int` containing the start time snapshot for slicing the data sequentially at run time, (3) `int` containing the end time snapshot for slicing the data sequentially at run time, and (4) a `list` containing the name of the variables in your data file. \n",
    "\n",
    "\n",
    "\n",
    "The required parameters are as follows:\n",
    "- `time_step`: time-sampling of the data (for now this must be constant)\n",
    "- `n_snapshots`: number of time snapshots\n",
    "- `block_dimension`: number of snapshots in each block, it is related to the maximum number of frequency one can extract: n_freq=n_block/{2}+1\n",
    "- `n_space_dims`: number of spatial dimensions\n",
    "- `n_variables`: number of variables\n",
    "- `n_DFT`: length of FFT blocks\n",
    "\n",
    "The optional parameters are as follows:\n",
    "- `overlap`: dimension of the overlap region between adjacent blocks in percentage (0 to 100)\n",
    "- `mean_type`: type of mean to be subtracted from the data (`longtime`, `blockwise` or `zero`)\n",
    "- `normalize_weights`: weights normalization by data variance\n",
    "- `normalize_data`: normalize data by variance\n",
    "- `n_modes_save`: number of modes to be saved\n",
    "- `conf_level`: calculate confidence level of modes\n",
    "- `reuse_blocks`: whether to attempt reusing FFT blocks previously computed (if found)\n",
    "- `savefft`: save FFT blocks to reuse them in the future (to save time)\n",
    "- `savedir`: where to save the data\n",
    "\n",
    "**Note that we do not set any parameter for the Weights adopted to compute th einner product in the SPOD calculation. In this case, the algorithm will use automatically uniform weighting (weighting equal 1), and it will prompt a warning stating the use of default uniform weighting.** \n",
    "\n",
    "In the dictionary 'params_emulation' the following variables, which allow to define some relevant parameters of a single layer neural network, are stored:\n",
    "- `network`: string, type of neural network. In the present tutorial 'lstm'\n",
    "- `epochs`: integer, number of epochs\n",
    "- `batch_size`: integer, batch size\n",
    "- `n_seq_in`: integer, dimension of input sequence \n",
    "- `n_seq_out`: integer, number of steps to predict\n",
    "- `n_neurons`: number of neurons in each layer\n",
    "- `dropout`: value of the dropout\n",
    "- `savedir`: string, name of the directory where results will be saved\n",
    "\n",
    "In the present test case we use 60 (`n_seq_in`=60) previous values of the coefficients in order to evaluate the next one (`n_seq_out`=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data_arrays['dt'][0,0]\n",
    "block_dimension = 64 * dt\n",
    "X = data_arrays[variables[0]].T\n",
    "t = dt * np.arange(0,X.shape[0]); t = t.T\n",
    "nt = t.shape[0]\n",
    "x1 = data_arrays['r'].T; x1 = x1[:,0]\n",
    "x2 = data_arrays['x'].T; x2 = x2[0,:]\n",
    "\n",
    "\n",
    "trainingDataRatio = 0.8 # ratio between training data number and total number of snapshots\n",
    "testingDataRatio = (1-trainingDataRatio)\n",
    "\n",
    "# parameters\n",
    "params = dict()\n",
    "\n",
    "# -- required parameters\n",
    "params['time_step'   \t   ] = dt \t\t\t\t\t\t# data time-sampling\n",
    "params['n_space_dims'    ] = 2\t\t\t\t\t\t\t# number of spatial dimensions (longitude and latitude)\n",
    "params['n_variables'     ] = 1\t\t\t\t\t\t\t# number of variables\n",
    "params['n_DFT'           ] = np.ceil(block_dimension / dt) # length of FFT blocks\n",
    "# -- optional parameters\n",
    "params['overlap'          ] = 50\t\t\t\t\t# dimension in percentage (1 to 100) of block overlap\n",
    "params['mean_type'        ] = 'blockwise' # type of mean to subtract to the data\n",
    "params['normalize_weights'] = False\t \t\t\t# normalization of weights by data variance\n",
    "params['normalize_data'   ] = False  \t\t\t# normalize data by data variance\n",
    "params['n_modes_save'     ] = 8  \t\t# modes to be saved\n",
    "params['conf_level'       ] = 0.95   \t\t\t# calculate confidence level\n",
    "params['savedir'          ] = os.path.join(CWD, 'results', Path(file).stem)\n",
    "params['reuse_blocks'] = False\n",
    "\n",
    "params_emulation = dict()\n",
    "\n",
    "params_emulation['network'     ] = 'lstm' \t\t\t\t\t\t# type of network\n",
    "params_emulation['epochs'      ] = 130 \t\t\t\t\t\t# number of epochs\n",
    "params_emulation['batch_size'  ] = 32\t\t\t\t\t\t\t# batch size\n",
    "params_emulation['n_seq_in'    ] = 60\t\t\t\t\t\t\t# dimension of input sequence \n",
    "params_emulation['n_seq_out'   ] = 1                          # number of steps to predict\n",
    "params_emulation['n_neurons'   ] = 25                          # number of neurons\n",
    "params_emulation['dropout'   ] = 0.15                          # dropout\n",
    "params_emulation['savedir'     ] = os.path.join(CWD, 'results', Path(file).stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SPOD modes and coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code are used for the initialization of variables. `X_train` and `X_test` are numpy data structures which contain the training set and the testing set respectively; therefore their dimensions are (`nt_train`, 20, 88) and (`nt_test`, 20, 88), being in this test case `nt_train`= $0.8\\cdot nt$ and nt_test=$0.2\\cdot nt$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training and testing database definition\n",
    "nt_train = int(trainingDataRatio * nt)\n",
    "X_train = X[:nt_train,:,:]\n",
    "nt_test = nt - nt_train\n",
    "X_test  = X[nt_train:,:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have initialized the variables above, we are ready to perform the SPOD analysisl at the end of this pahse we will have the SPOD modes computed for different ranges of frequencies.\n",
    "This phase can be computationally expansive and different techniques have been developed in order to handle data.\n",
    "If, as `data_handler`, we pass `False`, then we need to load the entire matrix of data into RAM, and that must comply with the **PySPOD** input data requirements (i.e. the dimension of the data matrix must correspond to (time $\\times$ spatial dimension shape $\\times$ number of variables). \n",
    "\n",
    "The arguments to the constructor are defined as follows:\n",
    "  - `params`: must be a dictionary and contains the parameters that we have just defined. \n",
    "  - `data_handler`: can be either `False` or a function handler. If it is a function handler, it must hold the function to read the data. The template for the function to read the data must have as first argument the data file, as second and third the time indices through which we will slice the data in time, and as fourth argument a list containing the name of the variables. See our data reader as an example and modify it according to your needs.\n",
    "  - `variables`: is a list containing our variables. \n",
    "  \n",
    " The `fit()` method returns a `PySPOD` object containg the results, the input arguments are a numpy array `X_train` and its first dimension (`nt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPOD analysis\n",
    "SPOD_analysis = SPOD_low_storage(\n",
    "\tparams=params, \n",
    "\tdata_handler=False, \n",
    "\tvariables=variables\n",
    "\t)\n",
    "\n",
    "# fit \n",
    "spod = SPOD_analysis.fit(X_train, nt=nt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transform function the pressure fluctuations are computed by subtracting the mean field from the snapshots. Then the SPOD modes are ordered by frequency and the coefficients are obtained by projecting the snapshots representing the pressure fluctuations onto the reduced POD basis obtained by gathering the most significant modes. In details, the `spod.transform` function accept as input\n",
    "- data: dataset on which the analysis is performed\n",
    "- nt: number of snapshots of the dataset 'data'\n",
    "\n",
    "and it returns a dictionary which contains the following keywords:\n",
    "- time_mean: the average in time of snapshots\n",
    "- phi_tilde: the first most significant modes ordered by frequency. These modes identify a reduced basis, significant for the case at hand.\n",
    "- coeffs: the coefficients obtained by projecting the snapshots onto the SPOD basis\n",
    "- reconstructed_data: snapshots reconstructed by superimposing the modes multiplied by the coefficients\n",
    "\n",
    "The same function is used for the testing set; the testing coefficients are obtaied by projecting the pressure fluctuations snapshots of`X_test` onto the SPOD basis previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "coeffs_train = spod.transform(X_train, nt=nt_train, T_lb=None, T_ub=None)\n",
    "coeffs_test  = spod.transform(X_test , nt=nt_test, T_lb=None, T_ub=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the latent space dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines are required in order to initialize the data structures needed to train the neural network and to store its output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of variables and structures\n",
    "n_modes = params['n_modes_save'] \n",
    "n_freq = spod._n_freq_r\n",
    "n_feature = coeffs_train['coeffs'].shape[0]\n",
    "\n",
    "data_train = np.zeros([n_freq,coeffs_train['coeffs'].shape[1]],dtype='complex')\n",
    "data_test = np.zeros([n_freq,coeffs_test['coeffs'].shape[1]],dtype='complex')\n",
    "coeffs_emul = np.zeros([coeffs_test['coeffs'].shape[0],coeffs_test['coeffs'].shape[1]],dtype='complex')\n",
    "coeffs_tmp = np.zeros([n_freq,coeffs_test['coeffs'].shape[1]],dtype='complex')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients previously evaluated can now be used for training a LSTM-based neural network.\n",
    "The Emulation constructor requires the following parameters:\n",
    "- params_emulation: dict containing the parameters described in the previous sections. They contain all the relevant data for creating a single-layer neural network with Dropout\n",
    "\n",
    "The neural network is initialized by calling `spod_emulation.model_initialize` that requires the data set which the network will be trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "spod_emulation = Emulation(params_emulation)\n",
    "\n",
    "# initialization of the network\n",
    "spod_emulation.model_initialize(data=data_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test case, we train `n_modes` separate neural networks, each of them contains `n_freq` features. Each network is associated to a different `idx` number.\n",
    "\n",
    "### Scale data\n",
    "It is a common practice to provide scaled input to the neural network. For this reason a scaler vector is computed by calling the function `compute_normalizationVectorReal`. Three different arguments can be used for defining the `normalizeMethod` variable:\n",
    "- localmax: each coefficient is scaled by its local maximum\n",
    "- globalmax: all the coefficients are scaled by the same value which represent the global maximum\n",
    "- None: no scaling is applied. The output vector contains ones.\n",
    "Once that the scaling vector is known, the scaling is applied both to the training dataset and to the testing one.\n",
    "\n",
    "### Train the network\n",
    "The training of the neural network is carried out by calling the `model_train` function. The following inputs are requested:\n",
    "- `idx`: integer, it is an identifier associated to the neural network. Thanks to this idx, more than one network can be trained in the same run and the weights can be stored in different files.\n",
    "- `data_train`: dataset used for the training\n",
    "- `data_valid`: dataset used for the validation\n",
    "- `plotHistory` (otpional): boolean, plot  the trainig history when set to `True`\n",
    "\n",
    "The resulting weights are saved on files.\n",
    "**If this function is commented and if a file with weights of a previously trained network with the same parameters and dimensions is present, the network is loaded automatically from file.**\n",
    "\n",
    "### Predict\n",
    "After that the neural network has been trained, predictions of the coefficients can be extracted with the aid of the `spod_emulation.model_inference` routine. This receives as inputs:\n",
    "- `idx`: integer, a value which identify a previously trained neural network (in this case 0, since we have only one neural network)\n",
    "- `data_input`: data which are used to start the prediction. This array can have an arbitary length. The first `n_seq_in` data are copied in the output vector and used for predicting the next `n_seq_out` steps\n",
    "\n",
    "The output consists in a vector which has the same dimensions of data_input and contains the predicted scaled coefficients.\n",
    "\n",
    "### Rescale data\n",
    "\n",
    "The predicted coefficients are then scaled back by calling `utils.denormalize_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(n_modes):\n",
    "\tidx_x = list(range(idx, n_feature, n_modes))\n",
    "    \n",
    "\t# scale data\n",
    "\tscaler  = \\\n",
    "\t\tutils.compute_normalizationVector(coeffs_train['coeffs'][idx_x,:],normalizeMethod='localmax')\n",
    "\tdata_train[:,:] = \\\n",
    "\t\tutils.normalize_data(coeffs_train['coeffs'][idx_x,:], normalizationVec=scaler)\n",
    "\tdata_test[:,:]  = \\\n",
    "\t\tutils.normalize_data(coeffs_test['coeffs'][idx_x,:], normalizationVec=scaler)\n",
    "\n",
    "    # train the network\n",
    "\tspod_emulation.model_train(idx,\n",
    "\t\tdata_train=data_train, \n",
    "\t\tdata_valid=data_test,\n",
    "\t\tplotHistory=False\n",
    "\t)\n",
    "    \n",
    "\t#predict \n",
    "\tcoeffs_tmp = spod_emulation.model_inference(\n",
    "\t\tidx,\n",
    "\t\tdata_input=data_test\n",
    "\t)\n",
    "\n",
    "\t# rescale data\n",
    "\tcoeffs_emul[idx_x,:] = utils.denormalize_data(coeffs_tmp, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the solution\n",
    "\n",
    "Now we have two distinct types of coefficients which we can use for reconstructing the snaptshots contained in X_test:\n",
    "- `coeffs_test`: the ones which were obtained by projecting the snapshot on the SPOD basis\n",
    "- `emul_coeffs`: the ones which were obtained with the prediction of the LSTM-based neural network.\n",
    "\n",
    "Fields are reconstructed and stored in a proper numpy array by calling `reconstruct_data` and providing the following input:\n",
    "- `coeffs`: the coefficients to be used for reconstructing the fields\n",
    "- `phi_tilde`: a structure containing the modes computed in the `transform` function\n",
    "- `time_mean`: the mean flow previously computed with the `transform` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct solutions\n",
    "phi_tilde = coeffs_train['phi_tilde']\n",
    "time_mean = coeffs_train['time_mean']\n",
    "\n",
    "proj_rec =spod.reconstruct_data(\n",
    "\t\tcoeffs=coeffs_test['coeffs'][:,:], \n",
    "\t\tphi_tilde=coeffs_train['phi_tilde'],\n",
    "\t\ttime_mean=coeffs_train['time_mean']\n",
    "\t)\n",
    "\n",
    "emulation_rec =spod.reconstruct_data(\n",
    "\t\tcoeffs=coeffs_emul, \n",
    "\t\tphi_tilde=coeffs_train['phi_tilde'],\n",
    "\t\ttime_mean=coeffs_train['time_mean']\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section of the code, some routines are placed for visulizing some results and computing the errors.\n",
    "\n",
    "`spod.printErrors`: compute and print L1, L2, and $L_{\\inf}$ average norm error for both the learning and the projection error. \n",
    "In intput the following input are required:\n",
    "- `field_test`: \"true\" solutions, it is a snapshot which belong to the original dataset \n",
    "- `field_proj`: fields reconstructed using the coeffs_test, from the comparison between this database and the one containing the true solutions we can evalute the projection error\n",
    "- `field_emul`: fields reconstructed using the coeffs_emul; from the comparison between this database and the field_proj we can evalute the learning error; from the comparison between this database and the one containing the true solutions we can evalute the total error.\n",
    "- `n_snaps`: number of snapshots on which the errors are evaluated \n",
    "- `n_offset`: offset\n",
    "\n",
    "`spod.plot_eigs()`: plot the **eigenvalues in the complex plane**, that is part of the `postprocessing` module. \n",
    "\n",
    "`spod.plot_eigs_vs_period()`: plot the eigevalues as a function of the characteristic period. We need access to the array 'spod.freq'\n",
    "\n",
    "`spod.plot_eigs_vs_frequency()`: plot the eigevalues as a function of the frequency. We need access to the array 'spod.freq'\n",
    "\n",
    "`spod.plot_compareTimeSeries`: compare time series, it is here used for comparing actual coefficients and the learned ones. It requires in input: two time series, two labels of the time series, legendLocation(otpional), and the filename (optional)\n",
    "\n",
    "`spod.generate_2D_subplot`: generate subpolts for visualizing the snapshots. It can show 1, 2 or 3 fields in the same frame. Inputs:\n",
    "- `title1`: title associated to the first snapshot\n",
    "- `title2`: title associated to the second snapshot (optional)\n",
    "- `title3`: title associated to the third snapshot (optional)\n",
    "- `var1`: 2D array of dimenions $20\\times88$ that we want to plot\n",
    "- `var2`: 2D array of dimenions $20\\times88$ that we want to plot (optional)\n",
    "- `var3`: 2D array of dimenions $20\\times88$ that we want to plot (optional)\n",
    "- `N_round`: number of decimals one wants to keep in the legend (optional)\n",
    "- `path`: path where one wants to store the results(optional)\n",
    "- `filename`: name of the file where to save the plot(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spod.printErrors(field_test=X_test, field_proj=proj_rec, field_emul=emulation_rec, n_snaps = 100, n_offset = 100)\n",
    "\n",
    "#\n",
    "spod.plot_eigs()\n",
    "spod.plot_eigs_vs_period()\n",
    "spod.plot_eigs_vs_frequency()\n",
    "\n",
    "#routines for visualization\n",
    "spod.plot_compareTimeSeries(\n",
    " \t  serie1= coeffs_test['coeffs'][0,:],\n",
    " \t  serie2= coeffs_emul[0,:],\n",
    " \t  label1='test',\n",
    " \t  label2='lstm',\n",
    " \t  legendLocation = 'upper left',\n",
    " \t  filename=None)\n",
    "\n",
    "spod.generate_2D_subplot(\n",
    "\ttitle1='True solution', \n",
    "\ttitle2='Projection-based solution', \n",
    "\ttitle3='LSTM-based solution',\n",
    "\tvar1=X_test[100,:,:], \n",
    "\tvar2=proj_rec[100,:,:,0], \n",
    "\tvar3=emulation_rec[100,:,:,0], \n",
    "\tN_round=2, path='CWD', filename=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
