{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ad0414",
   "metadata": {},
   "source": [
    "# MJO emulation via SPOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80c152",
   "metadata": {},
   "source": [
    "The Madden-Julian Oscillation (MJO) is an intraseasonal phenomenon that characterizes the tropical atmosphere. Its characteristic period varies between 30 and 90 days and it is basically due to a coupling between large-scale atmospheric circulation and deep convection. This pattern slowly propagates eastward with a speed of $4$ to $8$ $m s^{-1}$. MJO is a rather irregular phenomenon and this implies that the MJO can be seen at a large-scale level as a mix of multiple high-frequency, small-scale convective phenomena. The flow realizations which belong to the dataset represent the amount of total precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e2cda",
   "metadata": {},
   "source": [
    "The first step to analyze this dataset is to import the required libraries, including the custom libraries\n",
    "- `from pyspod.spod_low_storage import SPOD_low_storage`\n",
    "- `from pyspod.spod_low_ram     import SPOD_low_ram`\n",
    "- `from pyspod.spod_streaming   import SPOD_streaming`\n",
    "- `from pyspod.emulation   import Emulation`\n",
    "- `import pyspod.utils_weights as utils_weights`\n",
    "- `import pyspod.utils as utils`.  \n",
    "\n",
    "The first three contain different implementations of the SPOD algorithm, the first requiring low storage memory (intended for large RAM machines or small amount of data), the second requiring low RAM (intended for large dataset or small RAM machines), and the third being a streaming algorithm, that required little amount of memory (both storage and RAM) but runs typically slower than the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f15339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import xarray as xr\n",
    "import numpy  as np\n",
    "import opt_einsum as oe\n",
    "from pathlib  import Path\n",
    "\n",
    "# Current, parent and file paths\n",
    "CWD = os.getcwd()\n",
    "\n",
    "# Import library specific modules\n",
    "sys.path.append(os.path.join(CWD, \"../../../../\"))\n",
    "from pyspod.spod_low_storage import SPOD_low_storage\n",
    "from pyspod.spod_low_ram     import SPOD_low_ram\n",
    "from pyspod.spod_streaming   import SPOD_streaming\n",
    "from pyspod.emulation     import Emulation\n",
    "import pyspod.utils_weights as utils_weights\n",
    "import pyspod.postprocessing as post\n",
    "import pyspod.utils as utils  \n",
    "import mjo_plotting_utils as mjo_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb668a",
   "metadata": {},
   "source": [
    "The second step consists of downloading the data file `EI_1979_2017_TP228128_reduced5000.nc` from (...), and store it in the folder: `../../../../../test/data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59456bcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/57p2w1h12b915wh_7d77r9wc0000gn/T/ipykernel_18247/2574238552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../../../test/data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EI_1979_2017_TP228128_reduced5000.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vtij->tijv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "file = os.path.join('../../../../../test/data/', 'EI_1979_2017_TP228128_reduced5000.nc')\n",
    "ds = xr.open_dataset(file, chunks={\"time\": 10})\n",
    "\n",
    "da = ds.to_array()\n",
    "da = oe.contract('vtij->tijv', da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039db90",
   "metadata": {},
   "source": [
    "## Define global variables and global parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841dff7b",
   "metadata": {},
   "source": [
    "The data are stored in a matrix `X` and, to be suitable to the `PySPOD` library, it must have the following features:\n",
    "- first dimension must correspond to the number of time snapshots (5000 in our case)\n",
    "- last dimension should corresponds to the number of variables (1 in our case)\n",
    "- the remaining dimensions corresponds to the spatial dimensions (241, and 480 in our case, that correspond to radial and axial spatial coordinates).\n",
    "We note that in the present test case the data matrix `X` used is already in a shape that is suitable to `PySPOD`, as its dimension is:\n",
    "$$\\text{$X$ dimensions} = 5000 \\times 241 \\times 480 $$\n",
    "\n",
    "\n",
    "It is important to note at this point that we loaded all the required data into RAM, and stored it into a `numpy.ndarray`. We will later pass this array to the constructor of the `PySPOD` class for running our analysis. However, we could have used a different approach to load the data. In fact, the constructor to the `PySPOD` class accepts an argument called `data_handler`, that points to a function whose objective is to read the data at run time. This is particularly useful for large datasets, where it might be not possible to load all the data in RAM upfront. Therefore, in this case, we could simply define a data reader function as the following:\n",
    "\n",
    "```\n",
    "def read_data(data, t_0, t_end, variables): \n",
    "    ... implement here your method\n",
    "    data: path to the data file\n",
    "    t_0: start time slicing\n",
    "    t_end: end time slicing\n",
    "    variables: list with names of the variables\n",
    "\n",
    "    return X\n",
    "```\n",
    "\n",
    "and pass it to the `PySPOD` constructor under the argument `data_handler`. The path to the data file, will then be specified in place of the data, under the argument `X`. See below, when we setup the analysis and call the constructor for a more detailed explantion of the parameters `X` and `data_handler`. In summary, if `X` is a numpy.ndarray containing your data, `data_handler` is set to `False`, if `X` is a `str` containing the path to your data file, `data_handler` is a function that reads your data, and whose arguments must be: (1.) `str` containing the path to the data file, (2) `int` containing the start time snapshot for slicing the data sequentially at run time, (3) `int` containing the end time snapshot for slicing the data sequentially at run time, and (4) a `list` containing the name of the variables in your data file. \n",
    "\n",
    "\n",
    "\n",
    "The required parameters are as follows:\n",
    "- `time_step`: time-sampling of the data (for now this must be constant)\n",
    "- `n_snapshots`: number of time snapshots\n",
    "- `block_dimension`: number of snapshots in each block, it is related to the maximum number of frequency one can extract: n_freq=n_block/{2}+1\n",
    "- `n_space_dims`: number of spatial dimensions\n",
    "- `n_variables`: number of variables\n",
    "- `n_DFT`: length of FFT blocks\n",
    "\n",
    "The optional parameters are as follows:\n",
    "- `overlap`: dimension of the overlap region between adjacent blocks in percentage (0 to 100)\n",
    "- `mean_type`: type of mean to be subtracted from the data (`longtime`, `blockwise` or `zero`)\n",
    "- `normalize_weights`: weights normalization by data variance\n",
    "- `normalize_data`: normalize data by variance\n",
    "- `n_modes_save`: number of modes to be saved\n",
    "- `conf_level`: calculate confidence level of modes\n",
    "- `reuse_blocks`: whether to attempt reusing FFT blocks previously computed (if found)\n",
    "- `savefft`: save FFT blocks to reuse them in the future (to save time)\n",
    "- `savedir`: where to save the data\n",
    "\n",
    "**Note that we do not set any parameter for the Weights adopted to compute th einner product in the SPOD calculation. In this case, the algorithm will use automatically uniform weighting (weighting equal 1), and it will prompt a warning stating the use of default uniform weighting.** \n",
    "\n",
    "In the dictionary 'params_emulation' the following variables, which allow to define some relevant parameters of a single layer neural network, are stored:\n",
    "- `network`: string, type of neural network. In the present tutorial 'lstm'\n",
    "- `epochs`: integer, number of epochs\n",
    "- `batch_size`: integer, batch size\n",
    "- `n_seq_in`: integer, dimension of input sequence \n",
    "- `n_seq_out`: integer, number of steps to predict\n",
    "- `n_neurons`: number of neurons in each layer\n",
    "- `dropout`: value of the dropout\n",
    "- `savedir`: string, name of the directory where results will be saved\n",
    "\n",
    "In the present test case we use 40 (`n_seq_in`=40) previous values of the coefficients in order to evaluate the next one (`n_seq_out`=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95cc82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  ['1979-01-01T03:00:00.000000000' '1979-01-01T15:00:00.000000000'\n",
      " '1979-01-02T03:00:00.000000000' ... '1985-11-03T15:00:00.000000000'\n",
      " '1985-11-04T03:00:00.000000000' '1985-11-04T15:00:00.000000000']\n",
      "tSPOD =  ['1979-01-01T03:00:00.000000000' '1979-01-01T15:00:00.000000000'\n",
      " '1979-01-02T03:00:00.000000000' ... '1983-10-15T15:00:00.000000000'\n",
      " '1983-10-16T03:00:00.000000000' '1983-10-16T15:00:00.000000000']\n",
      "shape of t (time):  (5000,)\n",
      "shape of x1 (longitude):  (480,)\n",
      "shape of x2 (latitude) :  (241,)\n"
     ]
    }
   ],
   "source": [
    "# we extract time, longitude and latitude\n",
    "t = np.array(ds['time'])\n",
    "nt = t.shape[0]\n",
    "ntSPOD = int(0.7*len(t))\n",
    "tSPOD = t[:ntSPOD]\n",
    "print('t = ', t)\n",
    "print('tSPOD = ', tSPOD)\n",
    "x1 = np.array(ds['longitude'])\n",
    "x2 = np.array(ds['latitude'])\n",
    "print('shape of t (time): ', t.shape)\n",
    "print('shape of x1 (longitude): ', x1.shape)\n",
    "print('shape of x2 (latitude) : ', x2.shape)\n",
    "variables = ['tp']\n",
    "\n",
    "# define required and optional parameters for spod\n",
    "# 12-year monthly analysis\n",
    "dt_hours     = 12      \n",
    "period_hours = 24 * 365 \n",
    "params = {\n",
    "\t'time_step'   \t   : dt_hours,\n",
    "\t'n_snapshots' \t   : len(t),\n",
    "\t'n_snapshots_SPOD' : ntSPOD, # number of time snapshots for generating SPOD base\n",
    "\t'n_space_dims'\t   : 2,\n",
    "\t'n_variables' \t   : len(variables),\n",
    "\t'n_DFT'       \t   : int(np.ceil(period_hours / dt_hours)),\n",
    "\t'overlap'          : 0,\n",
    "\t'mean_type'        : 'longtime',\n",
    "\t'normalize_weights': False,\n",
    "\t'normalize_data'   : False,\n",
    "\t'n_modes_save'     : 1,\n",
    "\t'conf_level'       : 0.95,\n",
    "\t'reuse_blocks'     : False,\n",
    "\t'savedir'          : os.path.join(CWD, 'results', Path(file).stem)\n",
    "}\n",
    "\n",
    "params_emulation = dict()\n",
    "\n",
    "params_emulation['network'     ] = 'lstm' \t\t\t\t\t\t# type of network\n",
    "params_emulation['epochs'      ] = 10 \t\t\t\t\t\t# number of epochs\n",
    "params_emulation['batch_size'  ] = 32\t\t\t\t\t\t\t# batch size\n",
    "params_emulation['n_seq_in'    ] = 40\t\t\t\t\t\t\t# dimension of input sequence \n",
    "params_emulation['n_seq_out'   ] = 1                          # number of steps to predict\n",
    "params_emulation['n_neurons'   ] = 60                          # number of neurons\n",
    "params_emulation['dropout'   ] = 0.15                          # dropout\n",
    "params_emulation['savedir'     ] = os.path.join(CWD, 'results', Path(file).stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d469d",
   "metadata": {},
   "source": [
    "The following lines are used for computing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3277cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute weights ...\n",
      "elapsed time:  0.0026311874389648438 s.\n"
     ]
    }
   ],
   "source": [
    "# set weights\n",
    "st = time.time()\n",
    "print('compute weights ...')\n",
    "weights = utils_weights.geo_trapz_2D(\n",
    "\tx1_dim=x2.shape[0], \n",
    "\tx2_dim=x1.shape[0],\n",
    "\tn_vars=len(variables), \n",
    "\tR=1\n",
    ")\n",
    "print('elapsed time: ', time.time() - st, 's.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8ea0a",
   "metadata": {},
   "source": [
    "## Compute SPOD modes and coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce348ea",
   "metadata": {},
   "source": [
    "The following lines of code are used for the initialization of variables. `X_train` and `X_test` are numpy data structures which contain the training set and the testing set respectively; therefore their dimensions are (`nt_train`, 241, 480) and (`nt_test`, 241, 480), being in this test case `nt_train`= $0.75\\cdot nt$ and nt_test=$0.25\\cdot nt$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f264034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set blockwise mean\n",
    "params['mean_type'] = 'blockwise'\n",
    "params['reuse_blocks'] = False\n",
    "nt_train = int(0.75 * nt)\n",
    "nt_test = nt - nt_train\n",
    "X_train = da[:nt_train,:,:]\n",
    "X_test  = da[nt_train:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c498c",
   "metadata": {},
   "source": [
    "After we have initialized the variables above, we are ready to perform the SPOD analysisl at the end of this pahse we will have the SPOD modes computed for different ranges of frequencies.\n",
    "This phase can be computationally expansive and different techniques have been developed in order to handle data.\n",
    "If, as `data_handler`, we pass `False`, then we need to load the entire matrix of data into RAM, and that must comply with the **PySPOD** input data requirements (i.e. the dimension of the data matrix must correspond to (time $\\times$ spatial dimension shape $\\times$ number of variables). \n",
    "\n",
    "The arguments to the constructor are defined as follows:\n",
    "  - `params`: must be a dictionary and contains the parameters that we have just defined. \n",
    "  - `data_handler`: can be either `False` or a function handler. If it is a function handler, it must hold the function to read the data. The template for the function to read the data must have as first argument the data file, as second and third the time indices through which we will slice the data in time, and as fourth argument a list containing the name of the variables. See our data reader as an example and modify it according to your needs.\n",
    "  - `variables`: is a list containing our variables. \n",
    "  \n",
    " The `fit()` method returns a `PySPOD` object containg the results, the input arguments are a numpy array `X_train` and its first dimension (`nt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c31426ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Initialize data\n",
      "------------------------------------\n",
      "\n",
      "SPOD parameters\n",
      "------------------------------------\n",
      "Problem size               :  3.2320618629455615 GB. (double)\n",
      "No. of snapshots per block :  730\n",
      "Block overlap              :  0\n",
      "No. of blocks              :  5\n",
      "Windowing fct. (time)      :  hamming\n",
      "Weighting fct. (space)     :  geo_trapz_2D\n",
      "Mean                       :  blockwise\n",
      "Number of frequencies      :  366\n",
      "Time-step                  :  12\n",
      "Time snapshots             :  3750\n",
      "Space dimensions           :  2\n",
      "Number of variables        :  1\n",
      "Normalization weights      :  False\n",
      "Normalization data         :  False\n",
      "Number of modes to be saved:  1\n",
      "Confidence level for eigs  :  0.95\n",
      "Results to be saved in     :  /u/a/alario/pyspod_update/PySPOD/tutorials/climate/MJO/MJO_SPOD/results/EI_1979_2017_TP228128_reduced5000\n",
      "Save FFT blocks            :  False\n",
      "Reuse FFT blocks           :  False\n",
      "Spectrum type             :  one-sided (real-valued signal)\n",
      "------------------------------------\n",
      "\n",
      "------------------------------------\n",
      " \n",
      "Calculating temporal DFT (low_storage)\n",
      "--------------------------------------\n",
      "RAM available =  8.355312347412122\n",
      "RAM required  =  2.5166988372802774\n",
      "block 1/5 (0:730)\n",
      "block 2/5 (730:1460)\n",
      "block 3/5 (1460:2190)\n",
      "block 4/5 (2190:2920)\n",
      "block 5/5 (2920:3650)\n",
      "--------------------------------------\n",
      " \n",
      "Calculating SPOD (low_storage)\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing frequencies: 100%|██████████| 366/366 [00:24<00:00, 15.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      " \n",
      "Results saved in folder  /u/a/alario/pyspod_update/PySPOD/tutorials/climate/MJO/MJO_SPOD/results/EI_1979_2017_TP228128_reduced5000/nfft730_novlp0_nblks5\n",
      "Elapsed time:  92.94034457206726 s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SPOD analysis\n",
    "SPOD_analysis = SPOD_low_storage(\n",
    "\tparams=params, \n",
    "\tdata_handler=False, \n",
    "\tvariables=variables,\n",
    "\tweights=weights\n",
    "    )\n",
    "\n",
    "# Fit \n",
    "spod = SPOD_analysis.fit(X_train, nt=nt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1fb5d8",
   "metadata": {},
   "source": [
    "In the transform function the total precipitation fluctuations are computed by subtracting the mean field from the snapshots. Then the SPOD modes are ordered by frequency and the coefficients are obtained by projecting the snapshots representing the total precipitation fluctuations onto the reduced POD basis obtained by gathering the most significant modes. In details, the `spod.transform` function accept as input\n",
    "- data: dataset on which the analysis is performed\n",
    "- nt: number of snapshots of the dataset 'data'\n",
    "\n",
    "and it returns a dictionary which contains the following keywords:\n",
    "- time_mean: the average in time of snapshots\n",
    "- phi_tilde: the first most significant modes ordered by frequency. These modes identify a reduced basis, significant for the case at hand.\n",
    "- coeffs: the coefficients obtained by projecting the snapshots onto the SPOD basis\n",
    "- reconstructed_data: snapshots reconstructed by superimposing the modes multiplied by the coefficients\n",
    "\n",
    "The same function is used for the testing set; the testing coefficients are obtaied by projecting the total precipitation fluctuations snapshots of`X_test` onto the SPOD basis previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "spod_train = spod.transform(X_train, nt=nt_train, T_lb=30*24, T_ub=50*24)\n",
    "spod_test  = spod.transform(X_test , nt=nt_test, T_lb=30*24, T_ub=50*24)\n",
    "coeffs_train = spod_train['coeffs']\n",
    "coeffs_test = spod_test['coeffs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c583f5",
   "metadata": {},
   "source": [
    "## Learning the latent space dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4e30e",
   "metadata": {},
   "source": [
    "The following lines are required in order to initialize the data structures needed to train the neural network and to store its output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init variables\n",
    "n_modes = params['n_modes_save'] \n",
    "n_feature = coeffs_train.shape[0]\n",
    "n_freq = int(n_feature/n_modes)\n",
    "\n",
    "# init vectors\n",
    "data_train = np.zeros([n_freq,coeffs_train.shape[1]],dtype='complex')\n",
    "data_test = np.zeros([n_freq,coeffs_test.shape[1]],dtype='complex')\n",
    "coeffs = np.zeros([coeffs_test.shape[0],coeffs_test.shape[1]],dtype='complex')\n",
    "coeffs_tmp = np.zeros([n_freq,coeffs_test.shape[1]],dtype='complex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528bd8d",
   "metadata": {},
   "source": [
    "The coefficients previously evaluated can now be used for training a LSTM-based neural network.\n",
    "The Emulation constructor requires the following parameters:\n",
    "- params_emulation: dict containing the parameters described in the previous sections. They contain all the relevant data for creating a single-layer neural network with Dropout\n",
    "\n",
    "The neural network is initialized by calling `emulation.model_initialize` that requires the data set which the network will be trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "emulation = Emulation(params=params_emulation)\n",
    "# initialization of the network\n",
    "emulation.model_initialize(data=data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a09b11",
   "metadata": {},
   "source": [
    "In this test case, we train `n_modes` separate neural networks, each of them contains `n_freq` features. Each network is associated to a different `idx` number.\n",
    "\n",
    "### Scale data\n",
    "It is a common practice to provide scaled input to the neural network. For this reason a scaler vector is computed by calling the function `compute_normalizationVectorReal`. Three different arguments can be used for defining the `normalizeMethod` variable:\n",
    "- localmax: each coefficient is scaled by its local maximum\n",
    "- globalmax: all the coefficients are scaled by the same value which represent the global maximum\n",
    "- None: no scaling is applied. The output vector contains ones.\n",
    "Once that the scaling vector is known, the scaling is applied both to the training dataset and to the testing one.\n",
    "\n",
    "### Train the network\n",
    "The training of the neural network is carried out by calling the `model_train` function. The following inputs are requested:\n",
    "- `idx`: integer, it is an identifier associated to the neural network. Thanks to this idx, more than one network can be trained in the same run and the weights can be stored in different files.\n",
    "- `data_train`: dataset used for the training\n",
    "- `data_valid`: dataset used for the validation\n",
    "- `plotHistory` (otpional): boolean, plot  the trainig history when set to `True`\n",
    "\n",
    "The resulting weights are saved on files.\n",
    "**If this function is commented and if a file with weights of a previously trained network with the same parameters and dimensions is present, the network is loaded automatically from file.**\n",
    "\n",
    "### Predict\n",
    "After that the neural network has been trained, predictions of the coefficients can be extracted with the aid of the `spod_emulation.model_inference` routine. This receives as inputs:\n",
    "- `idx`: integer, a value which identify a previously trained neural network (in this case 0, since we have only one neural network)\n",
    "- `data_input`: data which are used to start the prediction. This array can have an arbitary length. The first `n_seq_in` data are copied in the output vector and used for predicting the next `n_seq_out` steps\n",
    "\n",
    "The output consists in a vector which has the same dimensions of data_input and contains the predicted scaled coefficients.\n",
    "\n",
    "### Rescale data\n",
    "\n",
    "The predicted coefficients are then scaled back by calling `utils.denormalize_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(n_modes):\n",
    "\t# get indexes of the idx-th mode\n",
    "\tidx_x = list(range(idx, n_feature, n_modes))\n",
    "\n",
    "\t# copy and normalize data \n",
    "\tscaler  = \\\n",
    "\t\tutils.compute_normalizationVector(coeffs_train[idx_x,:],normalizeMethod='localmax')\n",
    "\tdata_train[:,:] = \\\n",
    "\t\tutils.normalize_data(coeffs_train[idx_x,:], normalizationVec=scaler)\n",
    "\tdata_test[:,:]  = \\\n",
    "\t\tutils.normalize_data(coeffs_test[idx_x,:],\n",
    "\t\t\tnormalizationVec=scaler)\n",
    "\n",
    "\t#train the network\n",
    "\temulation.model_train(\n",
    "\t\tidx,\n",
    "\t\tdata_train=data_train, \n",
    "\t\tdata_valid=data_test,\n",
    "\t)\n",
    "\n",
    "\t#predict \n",
    "\tcoeffs_tmp = emulation.model_inference(\n",
    "\t\tidx,\n",
    "\t\tdata_input=data_test\n",
    "\t)\n",
    "\n",
    "\t# denormalize data\n",
    "\tcoeffs[idx_x,:] = utils.denormalize_data(coeffs_tmp, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9270a",
   "metadata": {},
   "source": [
    "\n",
    "Now we have two distinct types of coefficients which we can use for reconstructing the snaptshots contained in X_test:\n",
    "- `coeffs_test`: the ones which were obtained by projecting the snapshot on the SPOD basis\n",
    "- `emul_coeffs`: the ones which were obtained with the prediction of the LSTM-based neural network.\n",
    "\n",
    "Fields are reconstructed and stored in a proper numpy array by calling `reconstruct_data` and providing the following input:\n",
    "- `coeffs`: the coefficients to be used for reconstructing the fields\n",
    "- `phi_tilde`: a structure containing the modes computed in the `transform` function\n",
    "- `time_mean`: the mean flow previously computed with the `transform` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct data\n",
    "emulation_rec =spod.reconstruct_data(\n",
    "\t\tcoeffs=coeffs, \n",
    "\t\tphi_tilde=spod_train['phi_tilde'],\n",
    "\t\ttime_mean=spod_train['time_mean']\n",
    "\t)\n",
    "proj_rec =spod.reconstruct_data(\n",
    "\t\tcoeffs=spod_test['coeffs'][:,:], \n",
    "\t\tphi_tilde=spod_train['phi_tilde'],\n",
    "\t\ttime_mean=spod_train['time_mean']\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f7a43",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63fc0e",
   "metadata": {},
   "source": [
    "In the last section of the code, some routines are placed for visulizing some results and computing the errors.\n",
    "\n",
    "`pod.printErrors`: compute and print L1, L2, and $L_{\\inf}$ average norm error for both the learning and the projection error. \n",
    "In intput the following input are required:\n",
    "- `field_test`: \"true\" solutions, it is a snapshot which belong to the original dataset \n",
    "- `field_proj`: fields reconstructed using the coeffs_test, from the comparison between this database and the one containing the true solutions we can evalute the projection error\n",
    "- `field_emul`: fields reconstructed using the coeffs_emul; from the comparison between this database and the field_proj we can evalute the learning error; from the comparison between this database and the one containing the true solutions we can evalute the total error.\n",
    "- `n_snaps`: number of snapshots on which the errors are evaluated \n",
    "- `n_offset`: offset\n",
    "\n",
    "`pod.plot_compareTimeSeries`: compare time series, it is here used for comparing actual coefficients and the learned ones. It requires in input: two time series, two labels of the time series, legendLocation(otpional), and the filename (optional)\n",
    "\n",
    "`mjo_plot.plot_2D_2subplot`: generate subpolts for visualizing the snapshots. It can show 2 fields in the same frame. Inputs:\n",
    "- `title1`: title associated to the first snapshot\n",
    "- `title2`: title associated to the second snapshot  \n",
    "- `var1`: 2D array of dimenions $241\\times480$ that we want to plot\n",
    "- `var2`: 2D array of dimenions $241\\times480$ that we want to plot\n",
    "- `x1`: longitude\n",
    "- `x2`: latitude\n",
    "- `N_round`: number of decimals one wants to keep in the legend (optional)\n",
    "- `path`: path where one wants to store the results(optional)\n",
    "- `filename`: name of the file where to save the plot(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output and visulalization\n",
    "spod.plot_eigs_vs_period()\n",
    "mjo_plot.plot_2D_snap(snaps=X_train,\n",
    " \tsnap_idx=[100], vars_idx=[0], x1=x1-180, x2=x2)\n",
    "\n",
    "mjo_plot.plot_2D_2subplot(\n",
    "\ttitle1='Projection-based solution', \n",
    "\ttitle2='LSTM-based solution',\n",
    "\tvar1=proj_rec[100,:,:,0], \n",
    "\tvar2=emulation_rec[100,:,:,0], \n",
    "\tx1 = x1-180, x2 = x2,\n",
    "\tN_round=6, path='CWD', filename=None, coastlines='centred', maxVal = 0.002, minVal= -0.0001\n",
    "\t)\n",
    "\n",
    "spod.printErrors(field_test=X_test, field_proj=proj_rec, field_emul=emulation_rec, n_snaps = 1000, n_offset = 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
