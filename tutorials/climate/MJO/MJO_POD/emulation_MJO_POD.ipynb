{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MJO emulation via POD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Madden-Julian Oscillation (MJO) is an intraseasonal phenomenon that characterizes the tropical atmosphere. Its characteristic period varies between 30 and 90 days and it is basically due to a coupling between large-scale atmospheric circulation and deep convection. This pattern slowly propagates eastward with a speed of $4$ to $8$ $m s^{-1}$. MJO is a rather irregular phenomenon and this implies that the MJO can be seen at a large-scale level as a mix of multiple high-frequency, small-scale convective phenomena. The flow realizations which belong to the dataset represent the amount of total precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to anlyze this dataset is to import the required libraries, including the custom libraries\n",
    "- 'from pyspod.auxiliary.pod_standard import POD_standard'\n",
    "- 'from pyspod.auxiliary.emulation   import Emulation'\n",
    "- 'import pyspod.utils_weights as utils_weights'\n",
    "- 'import pyspod.auxiliary.utils_emulation as utils_emulation'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import xarray as xr\n",
    "import numpy  as np\n",
    "import opt_einsum as oe\n",
    "from pathlib  import Path\n",
    "\n",
    "# Current, parent and file paths\n",
    "CWD = os.getcwd()\n",
    "CF  = os.path.realpath(__file__)\n",
    "CFD = os.path.dirname(CF)\n",
    "\n",
    "# Import library specific modules\n",
    "sys.path.append(os.path.join(CFD, \"../../../../\"))\n",
    "from pyspod.spod_low_storage import SPOD_low_storage\n",
    "from pyspod.spod_low_ram     import SPOD_low_ram\n",
    "from pyspod.spod_streaming   import SPOD_streaming\n",
    "from pyspod.auxiliary.emulation     import Emulation\n",
    "import pyspod.utils_weights as utils_weights\n",
    "import pyspod.postprocessing as post\n",
    "import pyspod.auxiliary.utils_emulation as utils_emulation  \n",
    "import mjo_plotting_utils as mjo_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step consists of downloading the data file `EI_1979_2017_TP228128_reduced5000.nc` from (...), and store it in the folder: `../../../../../test/data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join('../../../../../pyspod/test/data/', 'EI_1979_2017_TP228128_reduced5000.nc')\n",
    "ds = xr.open_dataset(file, chunks={\"time\": 10})\n",
    "\n",
    "da = ds.to_array()\n",
    "da = oe.contract('vtij->tijv', da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables and global parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored in a matrix `X` and, to be suitable to the `PySPOD` library, it must have the following features:\n",
    "- first dimension must correspond to the number of time snapshots (5000 in our case)\n",
    "- last dimension should corresponds to the number of variables (1 in our case)\n",
    "- the remaining dimensions corresponds to the spatial dimensions (241, and 480 in our case, that correspond to radial and axial spatial coordinates).\n",
    "We note that in the present test case the data matrix `X` used is already in a shape that is suitable to `PySPOD`, as its dimension is:\n",
    "$$\\text{$X$ dimensions} = 5000 \\times 241 \\times 480 $$\n",
    "\n",
    "Other global variables and parameters are defined. In detail:\n",
    "- `nt`: integer, number of snapshots, read from the data_arrays\n",
    "- `t`: vector containing the time instants at which the flow realizations have been stored\n",
    "- `x1`: list of coordinate along the x1-axis (in this case x1='longitude')\n",
    "- `x2`: list of coordinates along the x2-axis(in this case x2='latitude')\n",
    "- `trainingDataRatio`: real, ratio between training data number and total number of snapshots\n",
    "\n",
    "In the dictionary 'params' the following variables are stored:\n",
    "- `dt`: time step\n",
    "- `n_space_dims`: integer, number of space dimensions, in our case 2\n",
    "- `n_variables`: integer, nr of variables, in our case 1\n",
    "- `n_modes_save`: integer, number of modes that are taken into account (and stored)\n",
    "- `savedir`: string, name of the directory where results will be saved\n",
    "- `normalize_weights` (optional): boolean which activate the normalization of weights by data variance\n",
    "- `normalize_data` (optional): boolean which normalize data by data variance\n",
    "\n",
    "**Note that we do not set any parameter for the Weights adopted to compute th einner product in the SPOD calculation. In this case, the algorithm will use automatically uniform weighting (weighting equal 1), and it will prompt a warning stating the use of default uniform weighting.** \n",
    "\n",
    "In the dictionary 'params_emulation' the following variables, which allow to define some relevant parameters of a single layer neural network, are stored:\n",
    "- `network`: string, type of neural network. In the present tutorial 'lstm'\n",
    "- `epochs`: integer, number of epochs\n",
    "- `batch_size`: integer, batch size\n",
    "- `n_seq_in`: integer, dimension of input sequence \n",
    "- `n_seq_out`: integer, number of steps to predict\n",
    "- `n_neurons`: number of neurons in each layer\n",
    "- `dropout`: value of the dropout\n",
    "- `savedir`: string, name of the directory where results will be saved\n",
    "\n",
    "In the present test case we use 40 (`n_seq_in`=40) previous values of the coefficients in order to evaluate the next one (`n_seq_out`=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract time, longitude and latitude\n",
    "t = np.array(ds['time'])\n",
    "nt = t.shape[0]\n",
    "xshape =  da[0,...,0].shape\n",
    "nx = da[0,...,0].size\n",
    "nv = 1 \n",
    "\n",
    "ntSPOD = int(0.7*len(t))\n",
    "tSPOD = t[:ntSPOD]\n",
    "print('t = ', t)\n",
    "print('tSPOD = ', tSPOD)\n",
    "x1 = np.array(ds['longitude'])\n",
    "x2 = np.array(ds['latitude'])\n",
    "print('shape of t (time): ', t.shape)\n",
    "print('shape of x1 (longitude): ', x1.shape)\n",
    "print('shape of x2 (latitude) : ', x2.shape)\n",
    "variables = ['tp']\n",
    "\n",
    "# define required and optional parameters for spod\n",
    "# 12-year monthly analysis\n",
    "dt_hours     = 12      \n",
    "period_hours = 24 * 365 \n",
    "params = {\n",
    "\t'time_step'   \t   : dt_hours,\n",
    "\t'n_snapshots' \t   : len(t),\n",
    "\t'n_snapshots_POD'  : ntSPOD, # number of time snapshots for generating SPOD base\n",
    "\t'n_space_dims'\t   : 2,\n",
    "\t'n_variables' \t   : len(variables),\n",
    "\t'mean_type'        : 'longtime',\n",
    "\t'normalize_weights': False,\n",
    "\t'normalize_data'   : False,\n",
    "\t'n_modes_save'     : 2,\n",
    "\t'savedir'          : os.path.join(CWD, 'results', Path(file).stem)\n",
    "}\n",
    "print('params \\n', params)\n",
    "\n",
    "params_emulation = dict()\n",
    "\n",
    "params_emulation['network'     ] = 'lstm' \t\t\t\t\t\t# type of network\n",
    "params_emulation['epochs'      ] = 100\t\t\t\t\t\t# number of epochs\n",
    "params_emulation['batch_size'  ] = 32\t\t\t\t\t\t\t# batch size\n",
    "params_emulation['n_seq_in'    ] = 40\t\t\t\t\t\t\t# dimension of input sequence \n",
    "params_emulation['n_seq_out'   ] = 1                          # number of steps to predict\n",
    "params_emulation['n_neurons'   ] = 60                          # number of neurons\n",
    "params_emulation['dropout'     ] = 0.15                          # dropout\n",
    "params_emulation['savedir'     ] = os.path.join(CWD, 'results', Path(file).stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization phase ends by setting the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weights\n",
    "st = time.time()\n",
    "weights = utils_weights.geo_trapz_2D(\n",
    "\tx1_dim=x2.shape[0], \n",
    "\tx2_dim=x1.shape[0],\n",
    "\tn_vars=len(variables), \n",
    "\tR=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute POD modes and coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code are used for the initialization of variables. `X_train` and `X_test` are numpy data structures which contain the training set and the testing set respectively; therefore their dimensions are (`nt_train`, 241, 480) and (`nt_test`, 241, 480), being in this test case `nt_train`= $0.75\\cdot nt$ and `nt_test`=$0.25\\cdot nt$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['mean_type'] = 'blockwise'\n",
    "params['reuse_blocks'] = False\n",
    "\n",
    "nt_train = int(0.75 * nt)\n",
    "nt_test = nt - nt_train\n",
    "X_train = da[:nt_train,:,:]\n",
    "X_test  = da[nt_train:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the data, defined the required and optional parameters, and allocated the testing and training structures we can perform the analysis. This step is accomplished by calling the `PySPOD` constructor`POD_standard(params=params, data_handler=False, variables=variables)` and the `fit` method, `POD_analysis.fit(data=X_train, nt=nt_train)`. \n",
    "\n",
    "The `POD_standard` constructor takes the following arguments :\n",
    "  - `params`: must be a dictionary and contains the parameters that we have just defined. \n",
    "  - `data_handler`: can be either `False` or a function handler. If it is a function handler, it must hold the function to read the data. The template for the function to read the data must have as first argument the data file, as second and third the time indices through which we will slice the data in time, and as fourth argument a list containing the name of the variables. See our data reader as an example and modify it according to your needs.\n",
    "  - `variables`: is a list containing our variables. \n",
    "The function `fit` returns a reference to the instance object on which it was called, given data and size of this dataset as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD analysis\n",
    "POD_analysis = POD_standard(\n",
    "\tparams=params, \n",
    "\tdata_handler=False, \n",
    "\tvariables=variables\n",
    "\t)\n",
    "\n",
    "# fit \n",
    "pod = POD_analysis.fit(data=X_train, nt=nt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transform function the pressure fluctuations are computed by subtracting the mean field from the snapshots. Then the POD modes are evaluated and the coefficients are obtained by projecting the snapshots representing the pressure fluctuations onto the reduced POD basis obtained by gathering the most significant modes. In details, the `pod.transform` function accept as input\n",
    "- `data`: dataset on which the analysis is performed\n",
    "- `nt`: number of snapshots of the dataset 'data'\n",
    "\n",
    "and it returns a dictionary which contains the following keywords:\n",
    "- `t_mean`: the average in time of snapshots\n",
    "- `phi_tilde`: the first most significant modes, i.e. the ones associated to the biggest n_save_modes eigenvalues. These modes identify a reduced basis, significant for the case at hand.\n",
    "- `coeffs`: the coefficients obtained by projection\n",
    "- `reconstructed_data`: snapshots reconstructed by superimposing the modes multiplied by the coefficients\n",
    "\n",
    "The coeffs_test is instead a vector which contains the coefficients which are evaluated by projecting the snapshots of the testing database onto the reduced POD basis previously computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_train = pod.transform(data=X_train, nt=nt_train)\n",
    "\n",
    "X_rearrange_test = np.reshape(X_test[:,:,:], [nt_test,pod.nv*pod.nx])\n",
    "for i in range(nt_test):\n",
    "\tX_rearrange_test[i,:] = np.squeeze(X_rearrange_test[i,:]) - np.squeeze(coeffs_train['t_mean'])\n",
    "coeffs_test = np.matmul(np.transpose(coeffs_train['phi_tilde']), X_rearrange_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the latent space dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines are required in order to initialize the data structures needed to train the neural network and to store its output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modes = params['n_modes_save'] \n",
    "n_feature = coeffs_train['coeffs'].shape[0]\n",
    "\n",
    "data_train = np.zeros([n_modes,coeffs_train['coeffs'].shape[1]],dtype='double')\n",
    "data_test = np.zeros([n_modes,coeffs_test.shape[1]],dtype='double')\n",
    "coeffs = np.zeros([coeffs_test.shape[0],coeffs_test.shape[1]],dtype='double')\n",
    "coeffs_tmp = np.zeros([n_modes,coeffs_test.shape[1]],dtype='double')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients previously evaluated can now be used for training a LSTM-based neural network.\n",
    "The Emulation constructor requires the following parameters:\n",
    "- `params_emulation`: dict containing the parameters described in the previous sections. They contain all the relevant data for creating a single-layer neural network with Dropout\n",
    "\n",
    "The neural network is initialized by calling `pod.model_initialize` that requires the data set which the network will be trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "pod_emulation = Emulation(params_emulation)\n",
    "\n",
    "# initialization of the network\n",
    "pod_emulation.model_initialize(data=data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to provide scaled input to the neural network. For this reason a scaler vector is computed by calling the function `utils_emulation.compute_normalization_vector_real`. Three different arguments can be used for defining the `normalize_method` variable:\n",
    "- `localmax`: each coefficient is scaled by its local maximum\n",
    "- `globalmax`: all the coefficients are scaled by the same value which represent the global maximum\n",
    "- `None`: no scaling is applied. The output vector contains ones.\n",
    "Once that the scaling vector is known, the scaling is applied both to the training dataset and to the testing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "\n",
    "# copy and normalize data \n",
    "scaler  = utils_emulation.compute_normalization_vector_real(coeffs_train['coeffs'][:,:],normalize_method='globalmax')\n",
    "data_train[:,:] = utils_emulation.normalize_data_real(coeffs_train['coeffs'][:,:], normalization_vec=scaler)\n",
    "data_test[:,:]  = utils_emulation.normalize_data_real(coeffs_test[:,:], normalization_vec=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the neural network is carried out by calling the `pod_emulation.model_train` function. The following inputs are requested:\n",
    "- `idx`: integer, it is an identifier associated to the neural network. Thanks to this idx, more than one network can be trained in the same run and the weights can be stored in different files.\n",
    "- `data_train`: dataset used for the training\n",
    "- `data_valid`: dataset used for the validation\n",
    "- `plotHistory` (otpional): boolean, plot  the trainig history when set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "pod_emulation.model_train(idx,\n",
    "\tdata_train=data_train, \n",
    "\tdata_valid=data_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that the neural network has been trained, predictions of the coefficients can be extracted with the aid of the `pod_emulation.model_inference` routine. This receives as inputs:\n",
    "- `idx`: integer, a value which identify a previously trained neural network (in this case 0, since we have only one neural network)\n",
    "- `data_input`: data which are used to start the prediction. This array can have an arbitary length. The first `n_seq_in` data are copied in the output vector and used for predicting the next `n_seq_out` steps\n",
    "\n",
    "The output consists in a vector which has the same dimensions of data_input and contains the predicted scaled coefficients.\n",
    "\n",
    "The predicted coefficients are then scaled back by calling `utils_emulation.denormalize_data_real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict \n",
    "coeffs_tmp = pod_emulation.model_inference(\n",
    "\tidx,\n",
    "\tdata_input=data_test\n",
    ")\n",
    "\n",
    "# denormalize data\n",
    "coeffs = utils_emulation.denormalize_data_real(coeffs_tmp, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two distinct types of coefficients which we can use for reconstructing the snaptshots contained in X_test:\n",
    "- `coeffs_test`: the ones which were obtained by projecting the snapshot on the POD basis\n",
    "- `emul_coeffs`: the ones which were obtained with the prediction of the LSTM-based neural network.\n",
    "\n",
    "Fields are reconstructed and stored in a proper numpy array by calling `reconstruct_data` and providing the following input:\n",
    "- `coeffs`: the coefficients to be used for reconstructing the fields\n",
    "- `phi_tilde`: a structure containing the modes computed in the `transform` function\n",
    "- `t_mean`: the mean flow previously computed with the `transform` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# reconstruct solutions\n",
    "\temulation_rec = pod.reconstruct_data(\n",
    "\t\t\tcoeffs=coeffs, \n",
    "\t\t\tphi_tilde=coeffs_train['phi_tilde'],\n",
    "\t\t\tt_mean=coeffs_train['t_mean']\n",
    "\t\t)\n",
    "\tproj_rec = pod.reconstruct_data(\n",
    "\t\t\tcoeffs=coeffs_test, \n",
    "\t\t\tphi_tilde=coeffs_train['phi_tilde'],\n",
    "\t\t\tt_mean=coeffs_train['t_mean']\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section of the code, some routines are placed for visulizing some results and computing the errors.\n",
    "\n",
    "`pod.printErrors`: compute and print L1, L2, and $L_{\\inf}$ average norm error for both the learning and the projection error. \n",
    "In intput the following input are required:\n",
    "- `field_test`: \"true\" solutions, it is a snapshot which belong to the original dataset \n",
    "- `field_proj`: fields reconstructed using the coeffs_test, from the comparison between this database and the one containing the true solutions we can evalute the projection error\n",
    "- `field_emul`: fields reconstructed using the coeffs_emul; from the comparison between this database and the field_proj we can evalute the learning error; from the comparison between this database and the one containing the true solutions we can evalute the total error.\n",
    "- `n_snaps`: number of snapshots on which the errors are evaluated \n",
    "- `n_offset`: offset\n",
    "\n",
    "`pod.plot_compareTimeSeries`: compare time series, it is here used for comparing actual coefficients and the learned ones. It requires in input: two time series, two labels of the time series, legendLocation(otpional), and the filename (optional)\n",
    "\n",
    "`mjo_plot.plot_2d_2subplot`: generate subpolts for visualizing the snapshots. It can show 2 fields in the same frame. Inputs:\n",
    "- `title1`: title associated to the first snapshot\n",
    "- `title2`: title associated to the second snapshot  \n",
    "- `var1`: 2D array of dimenions $241\\times480$ that we want to plot\n",
    "- `var2`: 2D array of dimenions $241\\times480$ that we want to plot\n",
    "- `x1`: longitude\n",
    "- `x2`: latitude\n",
    "- `N_round`: number of decimals one wants to keep in the legend (optional)\n",
    "- `path`: path where one wants to store the results(optional)\n",
    "- `filename`: name of the file where to save the plot(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mjo_plot.plot_2d_snap(snaps=X_train,\n",
    " \tsnap_idx=[100], vars_idx=[0], x1=x1-180, x2=x2)\n",
    "\n",
    "mjo_plot.plot_2d_2subplot(\n",
    "\ttitle1='Projection-based solution', \n",
    "\ttitle2='LSTM-based solution',\n",
    "\tvar1=proj_rec[100,:,:,0], \n",
    "\tvar2=emulation_rec[100,:,:,0], \n",
    "\tx1 = x1-180, x2 = x2,\n",
    "\tN_round=6, path='CWD', filename=None, coastlines='centred', maxVal = 0.002, minVal= -0.0001\n",
    "\t)\n",
    "\n",
    "pod.printErrors(field_test=X_test, field_proj=proj_rec, field_emul=emulation_rec, n_snaps = 1000, n_offset = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
